{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/bgefinetune/lib/python3.10/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/ubuntu/anaconda3/envs/bgefinetune/lib/python3.10/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/home/ubuntu/anaconda3/envs/bgefinetune/lib/python3.10/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /lib/x86_64-linux-gnu/libm.so.6: version `GLIBC_2.29' not found (required by /home/ubuntu/anaconda3/envs/bgefinetune/lib/python3.10/site-packages/libpyg.so)\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    " \n",
    "# Import dataset from PyTorch Geometric\n",
    "dataset = Planetoid(root=\".\", name=\"Cora\")\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "class CustomGATLayer(torch.nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, alpha=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 기본 선형 변환 (bias 없음)\n",
    "        self.linear = nn.Linear(dim_in, dim_out, bias=False)  # W\n",
    "        self.att_w = nn.Linear(dim_out*2, 1) # Attention score\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # 밀집 형태의 인접 행렬로 변환 (Self-loop 추가)\n",
    "        adjacency = to_dense_adj(edge_index)[0]\n",
    "        adj_matrix = adjacency + torch.eye(x.size(0), device=x.device)\n",
    "\n",
    "        # (1) 선형 변환\n",
    "        h = self.linear(x)  # X @ W.T\n",
    "\n",
    "        # 연결된 노드 쌍 추출\n",
    "        src, dst = adj_matrix.nonzero(as_tuple=True)  # (source, neighbor)\n",
    "\n",
    "        # (2) Attention score 계산\n",
    "        h_cat = torch.cat([h[src], h[dst]], dim=1)  # (N_edges, 2 * dim_out)\n",
    "        a = self.att_w(h_cat).squeeze() # attention score\n",
    "        e = self.leakyrelu(a)  # 활성화 함수 적용\n",
    "        \n",
    "        # (3) Softmax 정규화\n",
    "        E = torch.zeros_like(adjacency)  # (N, N) 크기의 0 행렬\n",
    "        E[src, dst] = e  # 인접한 노드에만 score 적용\n",
    "        T = 0.1\n",
    "        w_att = torch.softmax(E/T, dim=1)  # 소프트맥스 적용 => softmax 를 적용하면 크기가 너무 작아져 학습 X , Temperature 도입\n",
    "        \n",
    "        # (4) Attention 가중치 적용 후 메시지 전달\n",
    "        H = w_att @ h # 가중치 행렬 @ 특성 행렬\n",
    "        H = adj_matrix @ H # 메세지 전달\n",
    "        return H\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2708"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (gat1): CustomGATLayer(\n",
      "    (linear): Linear(in_features=1433, out_features=32, bias=False)\n",
      "    (att_w): Linear(in_features=64, out_features=1, bias=True)\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (gat2): CustomGATLayer(\n",
      "    (linear): Linear(in_features=32, out_features=7, bias=False)\n",
      "    (att_w): Linear(in_features=14, out_features=1, bias=True)\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      ")\n",
      "Epoch   0 | Train Loss: 2.413 | Train Acc: 15.71% | Val Loss: 2.19 | Val Acc: 15.80%\n",
      "Epoch  20 | Train Loss: 2.277 | Train Acc:  9.29% | Val Loss: 2.10 | Val Acc: 12.80%\n",
      "Epoch  40 | Train Loss: 1.976 | Train Acc: 14.29% | Val Loss: 2.04 | Val Acc: 12.20%\n",
      "Epoch  60 | Train Loss: 1.934 | Train Acc: 14.29% | Val Loss: 1.95 | Val Acc: 12.20%\n",
      "Epoch  80 | Train Loss: 1.950 | Train Acc: 17.86% | Val Loss: 2.04 | Val Acc: 9.40%\n",
      "Epoch 100 | Train Loss: 1.579 | Train Acc: 57.14% | Val Loss: 1.71 | Val Acc: 40.80%\n",
      "Epoch 120 | Train Loss: 0.464 | Train Acc: 86.43% | Val Loss: 1.39 | Val Acc: 56.60%\n",
      "Epoch 140 | Train Loss: 0.358 | Train Acc: 92.14% | Val Loss: 1.46 | Val Acc: 65.40%\n",
      "Epoch 160 | Train Loss: 0.746 | Train Acc: 90.00% | Val Loss: 2.14 | Val Acc: 59.20%\n",
      "Epoch 180 | Train Loss: 0.237 | Train Acc: 94.29% | Val Loss: 1.41 | Val Acc: 63.40%\n",
      "Epoch 200 | Train Loss: 0.155 | Train Acc: 96.43% | Val Loss: 1.60 | Val Acc: 68.00%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(14)\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv\n",
    "from torch.nn import Linear, Dropout\n",
    " \n",
    " \n",
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)\n",
    " \n",
    " \n",
    "class GAT(torch.nn.Module):\n",
    "    # multi-head GAT에서 첫번째 레이어의 head는 8개가 좋음. 이후는 달라짐\n",
    "    def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
    "        super().__init__()\n",
    "        self.gat1 = CustomGATLayer(dim_in, dim_h)\n",
    "        self.gat2 = CustomGATLayer(dim_h, dim_out)\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.6, training=self.training) # Preventing overfitting\n",
    "        h=x\n",
    "        h = self.gat1(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = F.dropout(h, p=0.6, training=self.training)\n",
    "        h = self.gat2(h, edge_index)\n",
    "        return F.log_softmax(h, dim=1)\n",
    " \n",
    "    def fit(self, data, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.01)\n",
    " \n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    " \n",
    "            if(epoch % 20 == 0):\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n",
    " \n",
    "    @torch.no_grad()\n",
    "    def test(self, data):\n",
    "        self.eval()\n",
    "        out = self(data.x, data.edge_index)\n",
    "        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "        return acc\n",
    " \n",
    "# Create the GAT model\n",
    "gat = GAT(dataset.num_features, 32, dataset.num_classes)\n",
    "print(gat)\n",
    " \n",
    "# Train\n",
    "gat.fit(data, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT test accuracy: 69.40%\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "acc = gat.test(data)\n",
    "print(f'GAT test accuracy: {acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi-head attention version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 멀티헤드 GAT\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils import to_dense_adj\n",
    "\n",
    "class MultiHeadGATLayer(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, num_heads=4, alpha=0.2, concat=True):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.concat = concat  # True: Concat 방식, False: Mean 방식\n",
    "        \n",
    "        # Multi-head Linear Transformation\n",
    "        self.linear = nn.ModuleList([\n",
    "            nn.Linear(dim_in, dim_out, bias=False) for _ in range(num_heads)\n",
    "        ])\n",
    "        \n",
    "        # 각 head마다 개별적인 attention score 계산을 위한 weight\n",
    "        self.att_w = nn.ModuleList([\n",
    "            nn.Linear(dim_out * 2, 1) for _ in range(num_heads)\n",
    "        ])\n",
    "\n",
    "        self.leakyrelu = nn.LeakyReLU(alpha)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        adjacency = to_dense_adj(edge_index)[0]\n",
    "        adj_matrix = adjacency + torch.eye(x.size(0), device=x.device)\n",
    "        \n",
    "        outputs = []\n",
    "        for i in range(self.num_heads):\n",
    "            h = self.linear[i](x)  # 각 head에 대해 다른 linear 변환 적용\n",
    "\n",
    "            # 인접 노드 쌍 (source, target) 추출\n",
    "            src, dst = adjacency.nonzero(as_tuple=True)\n",
    "\n",
    "            # Attention Score 계산\n",
    "            h_cat = torch.cat([h[src], h[dst]], dim=1)  # (N_edges, 2 * dim_out)\n",
    "            a = self.att_w[i](h_cat).squeeze()\n",
    "            e = self.leakyrelu(a)\n",
    "\n",
    "            # Softmax 적용 (인접 행렬 기반으로 score 배분)\n",
    "            E = torch.zeros_like(adjacency)\n",
    "            E[src, dst] = e\n",
    "            T = 0.1\n",
    "            w_att = torch.softmax(E/T, dim=1)\n",
    "\n",
    "            # 메시지 전달\n",
    "            H = w_att @ h\n",
    "            H = torch.sparse.mm(adj_matrix, H)\n",
    "            outputs.append(H)\n",
    "\n",
    "        # 여러 head의 출력을 병합\n",
    "        if self.concat:\n",
    "            H_final = torch.cat(outputs, dim=1)  # (N, dim_out * num_heads)\n",
    "        else:\n",
    "            H_final = torch.mean(torch.stack(outputs, dim=0), dim=0)  # (N, dim_out)\n",
    "\n",
    "        return H_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (gat1): MultiHeadGATLayer(\n",
      "    (linear): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=1433, out_features=32, bias=False)\n",
      "    )\n",
      "    (att_w): ModuleList(\n",
      "      (0-7): 8 x Linear(in_features=64, out_features=1, bias=True)\n",
      "    )\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      "  (gat2): MultiHeadGATLayer(\n",
      "    (linear): ModuleList(\n",
      "      (0): Linear(in_features=256, out_features=7, bias=False)\n",
      "    )\n",
      "    (att_w): ModuleList(\n",
      "      (0): Linear(in_features=14, out_features=1, bias=True)\n",
      "    )\n",
      "    (leakyrelu): LeakyReLU(negative_slope=0.2)\n",
      "  )\n",
      ")\n",
      "Epoch   0 | Train Loss: 1.971 | Train Acc: 14.29% | Val Loss: 1.95 | Val Acc: 11.40%\n",
      "Epoch  20 | Train Loss: 5.522 | Train Acc: 22.14% | Val Loss: 5.07 | Val Acc: 16.20%\n",
      "Epoch  40 | Train Loss: 1.786 | Train Acc: 39.29% | Val Loss: 2.14 | Val Acc: 28.60%\n",
      "Epoch  60 | Train Loss: 0.444 | Train Acc: 85.00% | Val Loss: 1.34 | Val Acc: 60.20%\n",
      "Epoch  80 | Train Loss: 0.124 | Train Acc: 97.14% | Val Loss: 1.25 | Val Acc: 70.60%\n",
      "Epoch 100 | Train Loss: 0.113 | Train Acc: 97.86% | Val Loss: 1.17 | Val Acc: 71.00%\n",
      "Epoch 120 | Train Loss: 0.067 | Train Acc: 98.57% | Val Loss: 0.99 | Val Acc: 78.00%\n",
      "Epoch 140 | Train Loss: 0.051 | Train Acc: 99.29% | Val Loss: 1.08 | Val Acc: 75.80%\n",
      "Epoch 160 | Train Loss: 0.055 | Train Acc: 99.29% | Val Loss: 1.20 | Val Acc: 74.00%\n",
      "Epoch 180 | Train Loss: 0.100 | Train Acc: 98.57% | Val Loss: 1.26 | Val Acc: 73.20%\n",
      "Epoch 200 | Train Loss: 0.039 | Train Acc: 100.00% | Val Loss: 1.14 | Val Acc: 75.40%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "torch.manual_seed(42)\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATv2Conv, GCNConv\n",
    "from torch.nn import Linear, Dropout\n",
    " \n",
    " \n",
    "def accuracy(y_pred, y_true):\n",
    "    \"\"\"Calculate accuracy.\"\"\"\n",
    "    return torch.sum(y_pred == y_true) / len(y_true)\n",
    " \n",
    " \n",
    "class GAT(torch.nn.Module):\n",
    "    # multi-head GAT에서 첫번째 레이어의 head는 8개가 좋음. 이후는 달라짐\n",
    "    def __init__(self, dim_in, dim_h, dim_out, heads=8):\n",
    "        super().__init__()\n",
    "        self.gat1 = MultiHeadGATLayer(dim_in, dim_h, num_heads=heads)\n",
    "        self.gat2 = MultiHeadGATLayer(dim_h*heads, dim_out, num_heads=1)\n",
    "    def forward(self, x, edge_index):\n",
    "        h = F.dropout(x, p=0.6, training=self.training) # Preventing overfitting\n",
    "        h=x\n",
    "        h = self.gat1(h, edge_index)\n",
    "        h = F.elu(h)\n",
    "        h = F.dropout(h, p=0.6, training=self.training)\n",
    "        h = self.gat2(h, edge_index)\n",
    "        return F.log_softmax(h, dim=1)\n",
    " \n",
    "    def fit(self, data, epochs):\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.01, weight_decay=0.01)\n",
    " \n",
    "        self.train()\n",
    "        for epoch in range(epochs+1):\n",
    "            optimizer.zero_grad()\n",
    "            out = self(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            acc = accuracy(out[data.train_mask].argmax(dim=1), data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    " \n",
    "            if(epoch % 20 == 0):\n",
    "                val_loss = criterion(out[data.val_mask], data.y[data.val_mask])\n",
    "                val_acc = accuracy(out[data.val_mask].argmax(dim=1), data.y[data.val_mask])\n",
    "                print(f'Epoch {epoch:>3} | Train Loss: {loss:.3f} | Train Acc: {acc*100:>5.2f}% | Val Loss: {val_loss:.2f} | Val Acc: {val_acc*100:.2f}%')\n",
    " \n",
    "    @torch.no_grad()\n",
    "    def test(self, data):\n",
    "        self.eval()\n",
    "        out = self(data.x, data.edge_index)\n",
    "        acc = accuracy(out.argmax(dim=1)[data.test_mask], data.y[data.test_mask])\n",
    "        return acc\n",
    " \n",
    "# Create the GAT model\n",
    "gat = GAT(dataset.num_features, 32, dataset.num_classes)\n",
    "print(gat)\n",
    " \n",
    "# Train\n",
    "gat.fit(data, epochs=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT test accuracy: 79.70%\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "acc = gat.test(data)\n",
    "print(f'GAT test accuracy: {acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bgefinetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
